---
title: "Ted Talk Analysis"
author: "Blain Morin"
date: "October 18, 2017"
output: pdf_document
---

```{r}
### Load required packages:
library(stringr)
library(tidyverse)
library(tidytext)
library(lubridate)
```

```{r}
### Read in metadata:
ted_main <- read_csv("~/skewl/text_analysis/text-mining-in-class-team6/TedTalks/ted_main.csv")

### Read in lecture transcripts:
transcripts <- read_csv("~/skewl/text_analysis/text-mining-in-class-team6/TedTalks/transcripts.csv")

### Fix url formatting so the two data sets match:
transcripts$url = str_replace_all(transcripts$url, pattern = "\r", replacement = "")
```

```{r}
### Combine data sets and change to token format for sentiment analysis:
combined = inner_join(ted_main, transcripts, by = "url")

transcripts_clean = combined %>% unnest_tokens(output = word, input = transcript)

### Add a total words per talk column:
transcripts_clean = transcripts_clean %>%
  group_by(name) %>%
  mutate(wordcount = n()) %>%
  ungroup()
```


### Which talks had the highest proportion of positive words?

```{r}
### Join with Bing lexicon:
transcripts_bing = transcripts_clean %>%
  inner_join(get_sentiments("bing"))
  
### Next, count the number of positive words in each talk
### Then, divide by the total words to obtain the proportion
### Then, output the top 20 most positive talks

transcripts_bing %>%
  filter(sentiment == "positive") %>%
  count(name, sentiment, wordcount) %>%
  mutate(positive_p = n / wordcount) %>%
  arrange(desc(positive_p)) %>%
  top_n(20)
```

### These results are unexpected. We can see that the most positive reult contains just 25 words. Let's rerun the code with a filter that removes the lowest 10 % of word counts.

```{r}


```




What was the top sentiment each year?






