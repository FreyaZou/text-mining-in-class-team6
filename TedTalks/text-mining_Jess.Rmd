---
title: "Text Analyis"
output: github_document
---

# Your mission

Perform text analysis.

## Okay, I need more information

Perform sentiment analysis or topic modeling using text analysis methods as demonstrated in the pre-class work and in the readings.

## Okay, I need even more information.

Do the above. Can't think of a data source?

- `gutenbergr`
- `AssociatedPress` from the `topicmodels` package
- `NYTimes` or `USCongress` from the `RTextTools` package
- Harry Potter Complete 7 Books text
    ```
    if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}

devtools::install_github("bradleyboehmke/harrypotter")
    ```
- [State of the Union speeches](https://pradeepadhokshaja.wordpress.com/2017/03/31/scraping-the-web-for-presdential-inaugural-addresses-using-rvest/)
- Scrape tweets using [`twitteR`](https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/)

Analyze the text for sentiment OR topic. **You do not need to do both**. The datacamp courses and [Tidy Text Mining with R](http://tidytextmining.com/) are good starting points for templates to perform this type of analysis, but feel free to *expand beyond these examples*.

# Timelines and Task


We will spend the next 2 weeks working on analyzing textual data in R. You will do the following:

- Start with some text based data.
- Clean data and prepare it for analysis
- Ask questions about the data
- Answer these questions with the data using tables and graphics
- Each group member must have their own unique question that they code the answer for. 
----------------------
```{r}
############### CLEANING #############################
#data from https://www.kaggle.com/rounakbanik/ted-talks#

library(readr)
library(dplyr)
library(tidytext)
library(stringr)
library(rebus)

##read in main data set
ted_main <- read_csv("~/text-mining-in-class-team6/TedTalks/ted_main.csv")

##read in transcripts
transcripts <- read_csv("~/text-mining-in-class-team6/TedTalks/transcripts.csv")

###combine data sets
full_data <- inner_join(ted_main, transcripts, by = "url")

###remove any text in the transcript that is surrounded by parenthesis

for (i in 1:nrow(full_data)){
  full_data[i, "transcript"] = 
    str_replace_all(
      full_data[i, "transcript"],
      pattern = "\\([^()]+\\)", 
      " "
    )
}

### Convert Unix time to a date:

transcripts_bing = transcripts_bing %>%
  mutate(year = year(as.Date(as.POSIXct(transcripts_bing$film_date, origin="1970-01-01"))))

######### get main rating for each talk #######

for(i in 1:nrow(full_data)) {
  rating_string <- str_sub(full_data$ratings[i], 2,-2)
  rating_vector <- unlist(strsplit(rating_string, split="}"))
  names <- str_extract_all(rating_vector, pattern = "'name': '" %R% one_or_more(WRD) %R% optional('-') %R% one_or_more(WRD), simplify = T)
  names <- str_replace(names, pattern = "'name': '", "")
  counts <- str_extract_all(rating_vector, pattern = "'count': " %R% one_or_more(DGT), simplify = T)
  counts <- str_replace(counts, pattern = "'count': ", "")
  full_data$max_rating[i] <- names[which.max(counts)]
}

transcripts_clean <- full_data %>% unnest_tokens(word, transcript)

sentiments <- transcripts_clean %>% inner_join(get_sentiments("bing")) 
                  #%>% filter(!word %in% c("like"))

############### ANALYZING #############################

word_counts <- sentiments %>% 
  group_by()
  count(max_rating, word)


top <- word_counts %>%
  group_by(max_rating) %>%
  top_n(12) %>%
  ungroup() %>%
  mutate(word = reorder(word, n))

# Use aes() to put words on the x-axis and n on the y-axis
ggplot(top, aes(word, n, fill = max_rating)) +
  # Make a bar chart with geom_col()
  geom_col(show.legend = FALSE) +
  facet_wrap(~max_rating, scales = "free") + 
  coord_flip()

```


